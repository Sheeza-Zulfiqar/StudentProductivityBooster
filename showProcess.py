# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'showProcess.ui'
#
# Created by: PyQt5 UI code generator 5.15.1
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.

import sys
import numpy as np
import cv2
import dlib
from PyQt5.QtGui import QImage,QPixmap
from PyQt5 import QtCore, QtGui, QtWidgets
from keras.models import load_model
from time import sleep
from keras.preprocessing.image import img_to_array
from keras.preprocessing import image
 


class Ui_Dialog(object):
    def onClicked(self):
        x =self.cmb_alg.currentText();
        y=self.cmb_cam.currentText();
   
        if x=='FaceDetection' and y=='WebCam': 
            self.faceDetectionUsingWebCam();
        if x=='FaceExpressionRecognition' and y=='WebCam': 
            self.emotionDetectionUsingWebCam()
        if x=='FaceExpressionRecognition' and y=='MobileCam': 
            self.emotionDetectionUsingMobileCam()    
        if x=='FaceDetection' and y=='MobileCam': 
            self.faceDetectionUsingMobileCam()       

        
             
    def stopClicked(self):
        self.logic=0
    def faceDetectionUsingWebCam(self):
        self.logic=1
         
        cap=cv2.VideoCapture(0,cv2.CAP_DSHOW)
        while (cap.isOpened()):
            ret,frame=cap.read()
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # Pass frame to our body classifier
            hogFaceDetector = dlib.get_frontal_face_detector()
            #print(hogFaceDetector)
            faces = hogFaceDetector(gray, 1)
            for (i, rect) in enumerate(faces):
                x = rect.left()
                y = rect.top()
                w = rect.right() - x
                h = rect.bottom() - y
                #draw a rectangle
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 255), 2)
                #cv2.imshow('Video',frame)
   
            if ret==True:
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0): 
                    break 
            else:
                print('return not found')   
        cap.release()
        cv2.destroyAllWindows()   
    def faceDetectionUsingMobileCam(self):

        self.logic=1
        cap=cv2.VideoCapture("http://192.168.0.102:4747/video") 
 
        while (cap.isOpened()):
            ret,frame=cap.read()
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # Pass frame to our body classifier
            hogFaceDetector = dlib.get_frontal_face_detector()
            #print(hogFaceDetector)
            faces = hogFaceDetector(gray, 1)
            for (i, rect) in enumerate(faces):
                x = rect.left()
                y = rect.top()
                w = rect.right() - x
                h = rect.bottom() - y
                #draw a rectangle
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 255), 2)
                #cv2.imshow('Video',frame)
   
            if ret==True:
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0): 
                    break 
            else:
                print('return not found')   
        cap.release()
        cv2.destroyAllWindows()           
    def emotionDetectionUsingWebCam(self):
        face_classifier = cv2.CascadeClassifier(r'C:/Users/Shiza/Documents/pyqt/fyp/haarcascade_frontalface_default.xml')
        classifier =load_model(r'C:/Users/Shiza/Documents/pyqt/fyp/Emotion_little_vgg.h5')
        #classifier =load_model(r'model25.h5')

        class_labels = ['Angry','Happy','Neutral','Sad','Surprise']
 
        angry=0 
        happy=0 
        sad=0
        surprise=0
        neutral=0
        summ=0

        s=set()
        
        self.logic=1
        
        #cap=cv2.VideoCapture("http://192.168.0.102:4747/video")
        cap=cv2.VideoCapture(0,cv2.CAP_DSHOW)
        while (cap.isOpened()):
            count=0
            
            ret,frame=cap.read()
            labels = []
            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
            faces = face_classifier.detectMultiScale(gray,1.3,5)
        
            for (x,y,w,h) in faces:

                count=count+1
                s.add(count)
                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)
                roi_gray = gray[y:y+h,x:x+w]
                roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)
            # rect,face,image = face_detector(frame)
            

                if np.sum([roi_gray])!=0:
                    roi = roi_gray.astype('float')/255.0
                    roi = img_to_array(roi)
                    roi = np.expand_dims(roi,axis=0)

                # make a prediction on the ROI, then lookup the class

                    preds = classifier.predict(roi)[0]
                    value=preds.argmax() 
                    if value==0:
                        angry=angry+1
                    elif value==1:
                        happy=happy+1
                    elif value==2:
                        neutral=neutral+1
                    elif value==3:
                        sad=sad+1
                    else:
                        surprise=surprise+1
                        
                    
                    label=class_labels[preds.argmax()]
                    #print(label)
                    label_position = (x,y)
                    cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
                else:
                    cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
            #cv2.imshow('Emotion Detector',frame)
            if ret==True:
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0):
                     
                    break 
            else:
                print('return not found')   


        summ =angry+happy+sad+surprise+neutral 
        print((angry/summ)*100,(happy/summ)*100,(neutral/summ)*100,(sad/summ)*100,(surprise/summ)*100)    
        cap.release()
        cv2.destroyAllWindows()             
    def emotionDetectionUsingMobileCam(self):

        face_classifier = cv2.CascadeClassifier(r'C:/Users/Shiza/Documents/pyqt/fyp/haarcascade_frontalface_default.xml')
        classifier =load_model(r'C:/Users/Shiza/Documents/pyqt/fyp/Emotion_little_vgg.h5')
        #classifier =load_model(r'model25.h5')

        class_labels = ['Angry','Happy','Neutral','Sad','Surprise']
 
        angry=0 
        happy=0 
        sad=0
        surprise=0
        neutral=0
        summ=0

        s=set()
        
        self.logic=1
        
        cap=cv2.VideoCapture("http://192.168.0.102:4747/video")
        #cap=cv2.VideoCapture(0,cv2.CAP_DSHOW)
        while (cap.isOpened()):
            count=0
            
            ret,frame=cap.read()
            labels = []
            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
            faces = face_classifier.detectMultiScale(gray,1.3,5)
        
            for (x,y,w,h) in faces:

                count=count+1
                s.add(count)
                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)
                roi_gray = gray[y:y+h,x:x+w]
                roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)
            # rect,face,image = face_detector(frame)
            

                if np.sum([roi_gray])!=0:
                    roi = roi_gray.astype('float')/255.0
                    roi = img_to_array(roi)
                    roi = np.expand_dims(roi,axis=0)

                # make a prediction on the ROI, then lookup the class

                    preds = classifier.predict(roi)[0]
                    value=preds.argmax() 
                    if value==0:
                        angry=angry+1
                    elif value==1:
                        happy=happy+1
                    elif value==2:
                        neutral=neutral+1
                    elif value==3:
                        sad=sad+1
                    else:
                        surprise=surprise+1
                        
                    
                    label=class_labels[preds.argmax()]
                    #print(label)
                    label_position = (x,y)
                    cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
                else:
                    cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
            #cv2.imshow('Emotion Detector',frame)
            if ret==True:
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0):
                     
                    break 
            else:
                print('return not found')   


        summ =angry+happy+sad+surprise+neutral 
        print((angry/summ)*100,(happy/summ)*100,(neutral/summ)*100,(sad/summ)*100,(surprise/summ)*100)    
        cap.release()
        cv2.destroyAllWindows()             

        
    def displayImage(self,img,window=1):
        qformat=QImage.Format_Indexed8
        if len(img.shape)==3:
            if (img.shape[2]) == 4:
                qformat=QImage.Format_RGBA888
            else:
                qformat=QImage.Format_RGB888 
         
        img=QImage(img,img.shape[1],img.shape[0],qformat)
        img=img.rgbSwapped()
        self.label.setPixmap(QPixmap.fromImage(img))    
        #self.label.setAlignment(Qt.AlignHCenter | Qt.AlignVCenter) 
        self.label.setAlignment(QtCore.Qt.AlignHCenter | QtCore.Qt.AlignVCenter)   

    def setupUi(self, Dialog):
        Dialog.setObjectName("Dialog")
        Dialog.resize(678, 443)
        Dialog.setStyleSheet("background-color: rgb(224, 249, 255); ")
        self.gridLayout_2 = QtWidgets.QGridLayout(Dialog)
        self.gridLayout_2.setObjectName("gridLayout_2")
        self.verticalLayout = QtWidgets.QVBoxLayout()
        self.verticalLayout.setObjectName("verticalLayout")
        self.horizontalLayout = QtWidgets.QHBoxLayout()
        self.horizontalLayout.setObjectName("horizontalLayout")
        self.cmb_alg = QtWidgets.QComboBox(Dialog)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.cmb_alg.setFont(font)
        self.cmb_alg.setObjectName("cmb_alg")
        self.cmb_alg.addItem("")
        self.cmb_alg.addItem("")
        self.horizontalLayout.addWidget(self.cmb_alg)
        self.verticalLayout.addLayout(self.horizontalLayout)
        self.gridLayout_2.addLayout(self.verticalLayout, 0, 1, 1, 1)
        self.cmb_cam = QtWidgets.QComboBox(Dialog)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.cmb_cam.setFont(font)
        self.cmb_cam.setObjectName("cmb_cam")
        self.cmb_cam.addItem("")
        self.cmb_cam.addItem("")
        self.gridLayout_2.addWidget(self.cmb_cam, 0, 2, 1, 1)
        self.START = QtWidgets.QPushButton(Dialog)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.START.setFont(font)
        self.START.setStyleSheet("background-color: rgb(170, 170, 127);")
        self.START.setObjectName("START")
        self.gridLayout_2.addWidget(self.START, 0, 3, 1, 1)
        self.STOP = QtWidgets.QPushButton(Dialog)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.STOP.setFont(font)
        self.STOP.setStyleSheet("background-color: rgb(170, 170, 127);")
        self.STOP.setObjectName("STOP")
        self.gridLayout_2.addWidget(self.STOP, 0, 4, 1, 1)
        self.label = QtWidgets.QLabel(Dialog)
        self.label.setObjectName("label")
        self.gridLayout_2.addWidget(self.label, 1, 1, 1, 4)

        self.retranslateUi(Dialog)
        QtCore.QMetaObject.connectSlotsByName(Dialog)
        self.logic=0
        
        self.START.clicked.connect(self.onClicked)
      
        self.STOP.clicked.connect(self.stopClicked)
      

    def retranslateUi(self, Dialog):
        _translate = QtCore.QCoreApplication.translate
        Dialog.setWindowTitle(_translate("Dialog", "Dialog"))
        self.cmb_alg.setItemText(0, _translate("Dialog", "FaceDetection"))
        self.cmb_alg.setItemText(1, _translate("Dialog", "FaceExpressionRecognition"))
        self.cmb_cam.setItemText(0, _translate("Dialog", "WebCam"))
        self.cmb_cam.setItemText(1, _translate("Dialog", "MobileCam"))
        self.START.setText(_translate("Dialog", "Start Process"))
        self.STOP.setText(_translate("Dialog", "Stop Process"))
        self.label.setText(_translate("Dialog", " "))


if __name__ == "__main__":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    Dialog = QtWidgets.QDialog()
    ui = Ui_Dialog()
    ui.setupUi(Dialog)
    Dialog.show()
    sys.exit(app.exec_())
