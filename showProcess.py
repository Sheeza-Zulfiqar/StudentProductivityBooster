# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'showProcess.ui'
#
# Created by: PyQt5 UI code generator 5.15.1
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.

import sys
import numpy as np
import cv2
import dlib
from PyQt5.QtGui import QImage,QPixmap
from PyQt5 import QtCore, QtGui, QtWidgets
from keras.models import load_model
from time import sleep
from keras.preprocessing.image import img_to_array
from keras.preprocessing import image
import math
from sklearn import neighbors
import os
import os.path
import pickle
from PIL import Image, ImageDraw
import face_recognition
from face_recognition.face_recognition_cli import image_files_in_folder


class Ui_Dialog(object):
    def onClicked(self):
        x =self.cmb_alg.currentText();
        y=self.cmb_cam.currentText();
   
        if x=='FaceDetection' and y=='WebCam': 
            self.faceDetectionUsingWebCam();
        if x=='FaceExpressionRecognition' and y=='WebCam': 
            self.emotionDetectionUsingWebCam()
        if x=='FaceExpressionRecognition' and y=='MobileCam': 
            self.emotionDetectionUsingMobileCam()    
        if x=='FaceDetection' and y=='MobileCam': 
            self.faceDetectionUsingMobileCam()     
        if x=='FaceRecognition' and y=='WebCam': 
            self.faceRecognitionUsingWebCam()        

    ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'JPG'}

    def faceRecognitionUsingWebCam(self):
        self.logic=1
        print("Training KNN classifier...")
        classifier = self.train("C:/Users/Shiza/TrainingImage", model_save_path="trained_knn_model.clf", n_neighbors=2)
        print("Training complete!")
        # process one frame in every 30 frames for speed

        process_this_frame = 29
        print('Setting cameras up...')
        # multiple cameras can be used with the format url = 'http://username:password@camera_ip:port'
        #url = 'http://admin:admin@192.168.0.106:8081/'
        cap = cv2.VideoCapture(0,cv2.CAP_DSHOW)
        while 1 > 0:
            ret, frame = cap.read()
            if ret:
                # Different resizing options can be chosen based on desired program runtime.
                # Image resizing for more stable streaming
                img = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
                process_this_frame = process_this_frame + 1
                if process_this_frame % 30 == 0:
                    predictions = self.predict(img, model_path="trained_knn_model.clf")
                frame = self.show_prediction_labels_on_image(frame, predictions)
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0):

                    break 
                #cv2.imshow('camera', frame)
                if ord('q') == cv2.waitKey(10):
                    cap.release()
                    cv2.destroyAllWindows()
                    exit(0) 
    def train(self,train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):
        """
        Trains a k-nearest neighbors classifier for face recognition.
        :param train_dir: directory that contains a sub-directory for each known person, with its name.
        (View in source code to see train_dir example tree structure)
        Structure:
            <train_dir>/
            ├── <person1>/
            │   ├── <somename1>.jpeg
            │   ├── <somename2>.jpeg
            │   ├── ...
            ├── <person2>/
            │   ├── <somename1>.jpeg
            │   └── <somename2>.jpeg
            └── ...
        :param model_save_path: (optional) path to save model on disk
        :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified
        :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree
        :param verbose: verbosity of training
        :return: returns knn classifier that was trained on the given data.
        """
        X = []
        y = []

        # Loop through each person in the training set
        for class_dir in os.listdir(train_dir):
            if not os.path.isdir(os.path.join(train_dir, class_dir)):
                continue

            # Loop through each training image for the current person
            for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):
                image = face_recognition.load_image_file(img_path)
                face_bounding_boxes = face_recognition.face_locations(image)

                if len(face_bounding_boxes) != 1:
                    # If there are no people (or too many people) in a training image, skip the image.
                    if verbose:
                        print("Image {} not suitable for training: {}".format(img_path, "Didn't find a face" if len(face_bounding_boxes) < 1 else "Found more than one face"))
                else:
                    # Add face encoding for current image to the training set
                    X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])
                    y.append(class_dir)

        # Determine how many neighbors to use for weighting in the KNN classifier
        if n_neighbors is None:
            n_neighbors = int(round(math.sqrt(len(X))))
            if verbose:
                print("Chose n_neighbors automatically:", n_neighbors)

        # Create and train the KNN classifier
        knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')
        knn_clf.fit(X, y)

        # Save the trained KNN classifier
        if model_save_path is not None:
            with open(model_save_path, 'wb') as f:
                pickle.dump(knn_clf, f)

        return knn_clf


    def predict(self,X_frame, knn_clf=None, model_path=None, distance_threshold=0.5):
        """
        Recognizes faces in given image using a trained KNN classifier
        :param X_frame: frame to do the prediction on.
        :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.
        :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.
        :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance
            of mis-classifying an unknown person as a known one.
        :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
            For faces of unrecognized persons, the name 'unknown' will be returned.
        """
        if knn_clf is None and model_path is None:
            raise Exception("Must supply knn classifier either thourgh knn_clf or model_path")

        # Load a trained KNN model (if one was passed in)
        if knn_clf is None:
            with open(model_path, 'rb') as f:
                knn_clf = pickle.load(f)

        X_face_locations = face_recognition.face_locations(X_frame)

        # If no faces are found in the image, return an empty result.
        if len(X_face_locations) == 0:
            return []

        # Find encodings for faces in the test image
        faces_encodings = face_recognition.face_encodings(X_frame, known_face_locations=X_face_locations)

        # Use the KNN model to find the best matches for the test face
        closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)
        are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]

        # Predict classes and remove classifications that aren't within the threshold
        return [(pred, loc) if rec else ("unknown", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]

    def show_prediction_labels_on_image(self,frame, predictions):
        """
        Shows the face recognition results visually.
        :param frame: frame to show the predictions on
        :param predictions: results of the predict function
        :return opencv suited image to be fitting with cv2.imshow fucntion:
        """
        pil_image = Image.fromarray(frame)
        draw = ImageDraw.Draw(pil_image)

        for name, (top, right, bottom, left) in predictions:
            # enlarge the predictions for the full sized image.
            top *= 2
            right *= 2
            bottom *= 2
            left *= 2
            # Draw a box around the face using the Pillow module
            draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))

            # There's a bug in Pillow where it blows up with non-UTF-8 text
            # when using the default bitmap font
            name = name.encode("UTF-8")

            # Draw a label with a name below the face
            text_width, text_height = draw.textsize(name)
            draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
            draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))

        # Remove the drawing library from memory as per the Pillow docs.
        del draw
        # Save image in open-cv format to be able to show it.

        opencvimage = np.array(pil_image)
        return opencvimage
        
             
    def stopClicked(self):
        self.logic=0
    def faceDetectionUsingWebCam(self):
        self.logic=1
         
        cap=cv2.VideoCapture(0,cv2.CAP_DSHOW)
        while (cap.isOpened()):
            ret,frame=cap.read()
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # Pass frame to our body classifier
            hogFaceDetector = dlib.get_frontal_face_detector()
            #print(hogFaceDetector)
            faces = hogFaceDetector(gray, 1)
            for (i, rect) in enumerate(faces):
                x = rect.left()
                y = rect.top()
                w = rect.right() - x
                h = rect.bottom() - y
                #draw a rectangle
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 255), 2)
                #cv2.imshow('Video',frame)
   
            if ret==True:
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0): 
                    break 
            else:
                print('return not found')   
        cap.release()
        cv2.destroyAllWindows()   
    def faceDetectionUsingMobileCam(self):

        self.logic=1
        cap=cv2.VideoCapture("http://192.168.0.102:4747/video") 
 
        while (cap.isOpened()):
            ret,frame=cap.read()
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # Pass frame to our body classifier
            hogFaceDetector = dlib.get_frontal_face_detector()
            #print(hogFaceDetector)
            faces = hogFaceDetector(gray, 1)
            for (i, rect) in enumerate(faces):
                x = rect.left()
                y = rect.top()
                w = rect.right() - x
                h = rect.bottom() - y
                #draw a rectangle
                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 255), 2)
                #cv2.imshow('Video',frame)
   
            if ret==True:
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0): 
                    break 
            else:
                print('return not found')   
        cap.release()
        cv2.destroyAllWindows()           
    def emotionDetectionUsingWebCam(self):
        face_classifier = cv2.CascadeClassifier(r'C:/Users/Shiza/Documents/pyqt/fyp/haarcascade_frontalface_default.xml')
        classifier =load_model(r'C:/Users/Shiza/Documents/pyqt/fyp/Emotion_little_vgg.h5')
        #classifier =load_model(r'model25.h5')

        class_labels = ['Angry','Happy','Neutral','Sad','Surprise']
 
        angry=0 
        happy=0 
        sad=0
        surprise=0
        neutral=0
        summ=0

        s=set()
        
        self.logic=1
        
        #cap=cv2.VideoCapture("http://192.168.0.102:4747/video")
        cap=cv2.VideoCapture(0,cv2.CAP_DSHOW)
        while (cap.isOpened()):
            count=0
            
            ret,frame=cap.read()
            labels = []
            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
            faces = face_classifier.detectMultiScale(gray,1.3,5)
        
            for (x,y,w,h) in faces:

                count=count+1
                s.add(count)
                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)
                roi_gray = gray[y:y+h,x:x+w]
                roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)
            # rect,face,image = face_detector(frame)
            

                if np.sum([roi_gray])!=0:
                    roi = roi_gray.astype('float')/255.0
                    roi = img_to_array(roi)
                    roi = np.expand_dims(roi,axis=0)

                # make a prediction on the ROI, then lookup the class

                    preds = classifier.predict(roi)[0]
                    value=preds.argmax() 
                    if value==0:
                        angry=angry+1
                    elif value==1:
                        happy=happy+1
                    elif value==2:
                        neutral=neutral+1
                    elif value==3:
                        sad=sad+1
                    else:
                        surprise=surprise+1
                        
                    
                    label=class_labels[preds.argmax()]
                    #print(label)
                    label_position = (x,y)
                    cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
                else:
                    cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
            #cv2.imshow('Emotion Detector',frame)
            if ret==True:
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0):
                     
                    break 
            else:
                print('return not found')   


        summ =angry+happy+sad+surprise+neutral 
        print((angry/summ)*100,(happy/summ)*100,(neutral/summ)*100,(sad/summ)*100,(surprise/summ)*100)    
        cap.release()
        cv2.destroyAllWindows()             
    def emotionDetectionUsingMobileCam(self):

        face_classifier = cv2.CascadeClassifier(r'C:/Users/Shiza/Documents/pyqt/fyp/haarcascade_frontalface_default.xml')
        classifier =load_model(r'C:/Users/Shiza/Documents/pyqt/fyp/Emotion_little_vgg.h5')
        #classifier =load_model(r'model25.h5')

        class_labels = ['Angry','Happy','Neutral','Sad','Surprise']
 
        angry=0 
        happy=0 
        sad=0
        surprise=0
        neutral=0
        summ=0

        s=set()
        
        self.logic=1
        
        cap=cv2.VideoCapture("http://192.168.0.102:4747/video")
        #cap=cv2.VideoCapture(0,cv2.CAP_DSHOW)
        while (cap.isOpened()):
            count=0
            
            ret,frame=cap.read()
            labels = []
            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
            faces = face_classifier.detectMultiScale(gray,1.3,5)
        
            for (x,y,w,h) in faces:

                count=count+1
                s.add(count)
                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)
                roi_gray = gray[y:y+h,x:x+w]
                roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)
            # rect,face,image = face_detector(frame)
            

                if np.sum([roi_gray])!=0:
                    roi = roi_gray.astype('float')/255.0
                    roi = img_to_array(roi)
                    roi = np.expand_dims(roi,axis=0)

                # make a prediction on the ROI, then lookup the class

                    preds = classifier.predict(roi)[0]
                    value=preds.argmax() 
                    if value==0:
                        angry=angry+1
                    elif value==1:
                        happy=happy+1
                    elif value==2:
                        neutral=neutral+1
                    elif value==3:
                        sad=sad+1
                    else:
                        surprise=surprise+1
                        
                    
                    label=class_labels[preds.argmax()]
                    #print(label)
                    label_position = (x,y)
                    cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
                else:
                    cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)
            #cv2.imshow('Emotion Detector',frame)
            if ret==True:
                self.displayImage(frame,1)
                cv2.waitKey()
                if(self.logic==0):
                     
                    break 
            else:
                print('return not found')   


        summ =angry+happy+sad+surprise+neutral 
        print((angry/summ)*100,(happy/summ)*100,(neutral/summ)*100,(sad/summ)*100,(surprise/summ)*100)    
        cap.release()
        cv2.destroyAllWindows()             

        
    def displayImage(self,img,window=1):
        qformat=QImage.Format_Indexed8
        if len(img.shape)==3:
            if (img.shape[2]) == 4:
                qformat=QImage.Format_RGBA888
            else:
                qformat=QImage.Format_RGB888 
         
        img=QImage(img,img.shape[1],img.shape[0],qformat)
        img=img.rgbSwapped()
        self.label.setPixmap(QPixmap.fromImage(img))    
        #self.label.setAlignment(Qt.AlignHCenter | Qt.AlignVCenter) 
        self.label.setAlignment(QtCore.Qt.AlignHCenter | QtCore.Qt.AlignVCenter)   

    def setupUi(self, Dialog):
        Dialog.setObjectName("Dialog")
        Dialog.resize(678, 443)
        Dialog.setStyleSheet("background-color: rgb(224, 249, 255); ")
        self.gridLayout_2 = QtWidgets.QGridLayout(Dialog)
        self.gridLayout_2.setObjectName("gridLayout_2")
        self.verticalLayout = QtWidgets.QVBoxLayout()
        self.verticalLayout.setObjectName("verticalLayout")
        self.horizontalLayout = QtWidgets.QHBoxLayout()
        self.horizontalLayout.setObjectName("horizontalLayout")
        self.cmb_alg = QtWidgets.QComboBox(Dialog)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.cmb_alg.setFont(font)
        self.cmb_alg.setObjectName("cmb_alg")
        self.cmb_alg.addItem("")
        self.cmb_alg.addItem("")
        self.cmb_alg.addItem("")
        self.horizontalLayout.addWidget(self.cmb_alg)
        self.verticalLayout.addLayout(self.horizontalLayout)
        self.gridLayout_2.addLayout(self.verticalLayout, 0, 1, 1, 1)
        self.cmb_cam = QtWidgets.QComboBox(Dialog)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.cmb_cam.setFont(font)
        self.cmb_cam.setObjectName("cmb_cam")
        self.cmb_cam.addItem("")
        self.cmb_cam.addItem("")
        self.gridLayout_2.addWidget(self.cmb_cam, 0, 2, 1, 1)
        self.START = QtWidgets.QPushButton(Dialog)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.START.setFont(font)
        self.START.setStyleSheet("background-color: rgb(170, 170, 127);")
        self.START.setObjectName("START")
        self.gridLayout_2.addWidget(self.START, 0, 3, 1, 1)
        self.STOP = QtWidgets.QPushButton(Dialog)
        font = QtGui.QFont()
        font.setPointSize(14)
        self.STOP.setFont(font)
        self.STOP.setStyleSheet("background-color: rgb(170, 170, 127);")
        self.STOP.setObjectName("STOP")
        self.gridLayout_2.addWidget(self.STOP, 0, 4, 1, 1)
        self.label = QtWidgets.QLabel(Dialog)
        self.label.setObjectName("label")
        self.gridLayout_2.addWidget(self.label, 1, 1, 1, 4)

        self.retranslateUi(Dialog)
        QtCore.QMetaObject.connectSlotsByName(Dialog)
        self.logic=0
        
        self.START.clicked.connect(self.onClicked)
      
        self.STOP.clicked.connect(self.stopClicked)

    def retranslateUi(self, Dialog):
        _translate = QtCore.QCoreApplication.translate
        Dialog.setWindowTitle(_translate("Dialog", "Dialog"))
        self.cmb_alg.setItemText(0, _translate("Dialog", "FaceDetection"))
        self.cmb_alg.setItemText(1, _translate("Dialog", "FaceRecognition"))
        self.cmb_alg.setItemText(2, _translate("Dialog", "FaceExpressionRecognition"))
        self.cmb_cam.setItemText(0, _translate("Dialog", "WebCam"))
        self.cmb_cam.setItemText(1, _translate("Dialog", "MobileCam"))
        self.START.setText(_translate("Dialog", "Start Process"))
        self.STOP.setText(_translate("Dialog", "Stop Process"))
        self.label.setText(_translate("Dialog", " "))


if __name__ == "__main__":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    Dialog = QtWidgets.QDialog()
    ui = Ui_Dialog()
    ui.setupUi(Dialog)
    Dialog.show()
    sys.exit(app.exec_())
